{
 "metadata": {
  "name": "",
  "signature": "sha256:eac991c1536e53d6b60a5c23f053b5f15c763737acc82143149c406ff6371968"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from types import MethodType\n",
      "def showMethods(what):\n",
      "    \"\"\"Prints out all non-private methods of class-instance what.\"\"\"\n",
      "    for attr in dir(what):\n",
      "        if attr.startswith(\"_\"):\n",
      "            continue\n",
      "        if type(getattr(what, attr)) == MethodType:\n",
      "            print attr, getattr(what, attr).__doc__, \"\\n\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "showMethods(sc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "accumulator \n",
        "        Create an L{Accumulator} with the given initial value, using a given\n",
        "        L{AccumulatorParam} helper object to define how to add values of the\n",
        "        data type if provided. Default AccumulatorParams are used for integers\n",
        "        and floating-point numbers if you do not provide one. For other types,\n",
        "        a custom AccumulatorParam can be used.\n",
        "         \n",
        "\n",
        "addFile \n",
        "        Add a file to be downloaded with this Spark job on every node.\n",
        "        The C{path} passed can be either a local file, a file in HDFS\n",
        "        (or other Hadoop-supported filesystems), or an HTTP, HTTPS or\n",
        "        FTP URI.\n",
        "\n",
        "        To access the file in Spark jobs, use\n",
        "        L{SparkFiles.get(fileName)<pyspark.files.SparkFiles.get>} with the\n",
        "        filename to find its download location.\n",
        "\n",
        "        >>> from pyspark import SparkFiles\n",
        "        >>> path = os.path.join(tempdir, \"test.txt\")\n",
        "        >>> with open(path, \"w\") as testFile:\n",
        "        ...    testFile.write(\"100\")\n",
        "        >>> sc.addFile(path)\n",
        "        >>> def func(iterator):\n",
        "        ...    with open(SparkFiles.get(\"test.txt\")) as testFile:\n",
        "        ...        fileVal = int(testFile.readline())\n",
        "        ...        return [x * fileVal for x in iterator]\n",
        "        >>> sc.parallelize([1, 2, 3, 4]).mapPartitions(func).collect()\n",
        "        [100, 200, 300, 400]\n",
        "         \n",
        "\n",
        "addPyFile \n",
        "        Add a .py or .zip dependency for all tasks to be executed on this\n",
        "        SparkContext in the future.  The C{path} passed can be either a local\n",
        "        file, a file in HDFS (or other Hadoop-supported filesystems), or an\n",
        "        HTTP, HTTPS or FTP URI.\n",
        "         \n",
        "\n",
        "binaryFiles \n",
        "        .. note:: Experimental\n",
        "\n",
        "        Read a directory of binary files from HDFS, a local file system\n",
        "        (available on all nodes), or any Hadoop-supported file system URI\n",
        "        as a byte array. Each file is read as a single record and returned\n",
        "        in a key-value pair, where the key is the path of each file, the\n",
        "        value is the content of each file.\n",
        "\n",
        "        Note: Small files are preferred, large file is also allowable, but\n",
        "        may cause bad performance.\n",
        "         \n",
        "\n",
        "binaryRecords \n",
        "        .. note:: Experimental\n",
        "\n",
        "        Load data from a flat binary file, assuming each record is a set of numbers\n",
        "        with the specified numerical format (see ByteBuffer), and the number of\n",
        "        bytes per record is constant.\n",
        "\n",
        "        :param path: Directory to the input data files\n",
        "        :param recordLength: The length at which to split the records\n",
        "         \n",
        "\n",
        "broadcast \n",
        "        Broadcast a read-only variable to the cluster, returning a\n",
        "        L{Broadcast<pyspark.broadcast.Broadcast>}\n",
        "        object for reading it in distributed functions. The variable will\n",
        "        be sent to each cluster only once.\n",
        "         \n",
        "\n",
        "cancelAllJobs \n",
        "        Cancel all jobs that have been scheduled or are running.\n",
        "         \n",
        "\n",
        "cancelJobGroup \n",
        "        Cancel active jobs for the specified group. See L{SparkContext.setJobGroup}\n",
        "        for more information.\n",
        "         \n",
        "\n",
        "clearFiles \n",
        "        Clear the job's list of files added by L{addFile} or L{addPyFile} so\n",
        "        that they do not get downloaded to any new nodes.\n",
        "         \n",
        "\n",
        "dump_profiles  Dump the profile stats into directory `path`\n",
        "         \n",
        "\n",
        "getLocalProperty \n",
        "        Get a local property set in this thread, or null if it is missing. See\n",
        "        L{setLocalProperty}\n",
        "         \n",
        "\n",
        "hadoopFile \n",
        "        Read an 'old' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
        "        a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
        "        The mechanism is the same as for sc.sequenceFile.\n",
        "\n",
        "        A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
        "        Configuration in Java.\n",
        "\n",
        "        :param path: path to Hadoop file\n",
        "        :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
        "               (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
        "        :param keyClass: fully qualified classname of key Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.Text\")\n",
        "        :param valueClass: fully qualified classname of value Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "        :param conf: Hadoop configuration, passed in as a dict\n",
        "               (None by default)\n",
        "        :param batchSize: The number of Python objects represented as a single\n",
        "               Java object. (default 0, choose batchSize automatically)\n",
        "         \n",
        "\n",
        "hadoopRDD \n",
        "        Read an 'old' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
        "        Hadoop configuration, which is passed in as a Python dict.\n",
        "        This will be converted into a Configuration in Java.\n",
        "        The mechanism is the same as for sc.sequenceFile.\n",
        "\n",
        "        :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
        "               (e.g. \"org.apache.hadoop.mapred.TextInputFormat\")\n",
        "        :param keyClass: fully qualified classname of key Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.Text\")\n",
        "        :param valueClass: fully qualified classname of value Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "        :param conf: Hadoop configuration, passed in as a dict\n",
        "               (None by default)\n",
        "        :param batchSize: The number of Python objects represented as a single\n",
        "               Java object. (default 0, choose batchSize automatically)\n",
        "         \n",
        "\n",
        "newAPIHadoopFile \n",
        "        Read a 'new API' Hadoop InputFormat with arbitrary key and value class from HDFS,\n",
        "        a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
        "        The mechanism is the same as for sc.sequenceFile.\n",
        "\n",
        "        A Hadoop configuration can be passed in as a Python dict. This will be converted into a\n",
        "        Configuration in Java\n",
        "\n",
        "        :param path: path to Hadoop file\n",
        "        :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
        "               (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
        "        :param keyClass: fully qualified classname of key Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.Text\")\n",
        "        :param valueClass: fully qualified classname of value Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "        :param conf: Hadoop configuration, passed in as a dict\n",
        "               (None by default)\n",
        "        :param batchSize: The number of Python objects represented as a single\n",
        "               Java object. (default 0, choose batchSize automatically)\n",
        "         \n",
        "\n",
        "newAPIHadoopRDD \n",
        "        Read a 'new API' Hadoop InputFormat with arbitrary key and value class, from an arbitrary\n",
        "        Hadoop configuration, which is passed in as a Python dict.\n",
        "        This will be converted into a Configuration in Java.\n",
        "        The mechanism is the same as for sc.sequenceFile.\n",
        "\n",
        "        :param inputFormatClass: fully qualified classname of Hadoop InputFormat\n",
        "               (e.g. \"org.apache.hadoop.mapreduce.lib.input.TextInputFormat\")\n",
        "        :param keyClass: fully qualified classname of key Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.Text\")\n",
        "        :param valueClass: fully qualified classname of value Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "        :param conf: Hadoop configuration, passed in as a dict\n",
        "               (None by default)\n",
        "        :param batchSize: The number of Python objects represented as a single\n",
        "               Java object. (default 0, choose batchSize automatically)\n",
        "         \n",
        "\n",
        "parallelize \n",
        "        Distribute a local Python collection to form an RDD. Using xrange\n",
        "        is recommended if the input represents a range for performance.\n",
        "\n",
        "        >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
        "        [[0], [2], [3], [4], [6]]\n",
        "        >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
        "        [[], [0], [], [2], [4]]\n",
        "         \n",
        "\n",
        "pickleFile \n",
        "        Load an RDD previously saved using L{RDD.saveAsPickleFile} method.\n",
        "\n",
        "        >>> tmpFile = NamedTemporaryFile(delete=True)\n",
        "        >>> tmpFile.close()\n",
        "        >>> sc.parallelize(range(10)).saveAsPickleFile(tmpFile.name, 5)\n",
        "        >>> sorted(sc.pickleFile(tmpFile.name, 3).collect())\n",
        "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "         \n",
        "\n",
        "runJob \n",
        "        Executes the given partitionFunc on the specified set of partitions,\n",
        "        returning the result as an array of elements.\n",
        "\n",
        "        If 'partitions' is not specified, this will run over all partitions.\n",
        "\n",
        "        >>> myRDD = sc.parallelize(range(6), 3)\n",
        "        >>> sc.runJob(myRDD, lambda part: [x * x for x in part])\n",
        "        [0, 1, 4, 9, 16, 25]\n",
        "\n",
        "        >>> myRDD = sc.parallelize(range(6), 3)\n",
        "        >>> sc.runJob(myRDD, lambda part: [x * x for x in part], [0, 2], True)\n",
        "        [0, 1, 16, 25]\n",
        "         \n",
        "\n",
        "sequenceFile \n",
        "        Read a Hadoop SequenceFile with arbitrary key and value Writable class from HDFS,\n",
        "        a local file system (available on all nodes), or any Hadoop-supported file system URI.\n",
        "        The mechanism is as follows:\n",
        "\n",
        "            1. A Java RDD is created from the SequenceFile or other InputFormat, and the key\n",
        "               and value Writable classes\n",
        "            2. Serialization is attempted via Pyrolite pickling\n",
        "            3. If this fails, the fallback is to call 'toString' on each key and value\n",
        "            4. C{PickleSerializer} is used to deserialize pickled objects on the Python side\n",
        "\n",
        "        :param path: path to sequncefile\n",
        "        :param keyClass: fully qualified classname of key Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.Text\")\n",
        "        :param valueClass: fully qualified classname of value Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.LongWritable\")\n",
        "        :param keyConverter:\n",
        "        :param valueConverter:\n",
        "        :param minSplits: minimum splits in dataset\n",
        "               (default min(2, sc.defaultParallelism))\n",
        "        :param batchSize: The number of Python objects represented as a single\n",
        "               Java object. (default 0, choose batchSize automatically)\n",
        "         \n",
        "\n",
        "setCheckpointDir \n",
        "        Set the directory under which RDDs are going to be checkpointed. The\n",
        "        directory must be a HDFS path if running on a cluster.\n",
        "         \n",
        "\n",
        "setJobGroup \n",
        "        Assigns a group ID to all the jobs started by this thread until the group ID is set to a\n",
        "        different value or cleared.\n",
        "\n",
        "        Often, a unit of execution in an application consists of multiple Spark actions or jobs.\n",
        "        Application programmers can use this method to group all those jobs together and give a\n",
        "        group description. Once set, the Spark web UI will associate such jobs with this group.\n",
        "\n",
        "        The application can use L{SparkContext.cancelJobGroup} to cancel all\n",
        "        running jobs in this group.\n",
        "\n",
        "        >>> import thread, threading\n",
        "        >>> from time import sleep\n",
        "        >>> result = \"Not Set\"\n",
        "        >>> lock = threading.Lock()\n",
        "        >>> def map_func(x):\n",
        "        ...     sleep(100)\n",
        "        ...     raise Exception(\"Task should have been cancelled\")\n",
        "        >>> def start_job(x):\n",
        "        ...     global result\n",
        "        ...     try:\n",
        "        ...         sc.setJobGroup(\"job_to_cancel\", \"some description\")\n",
        "        ...         result = sc.parallelize(range(x)).map(map_func).collect()\n",
        "        ...     except Exception as e:\n",
        "        ...         result = \"Cancelled\"\n",
        "        ...     lock.release()\n",
        "        >>> def stop_job():\n",
        "        ...     sleep(5)\n",
        "        ...     sc.cancelJobGroup(\"job_to_cancel\")\n",
        "        >>> supress = lock.acquire()\n",
        "        >>> supress = thread.start_new_thread(start_job, (10,))\n",
        "        >>> supress = thread.start_new_thread(stop_job, tuple())\n",
        "        >>> supress = lock.acquire()\n",
        "        >>> print result\n",
        "        Cancelled\n",
        "\n",
        "        If interruptOnCancel is set to true for the job group, then job cancellation will result\n",
        "        in Thread.interrupt() being called on the job's executor threads. This is useful to help\n",
        "        ensure that the tasks are actually stopped in a timely manner, but is off by default due\n",
        "        to HDFS-1208, where HDFS may respond to Thread.interrupt() by marking nodes as dead.\n",
        "         \n",
        "\n",
        "setLocalProperty \n",
        "        Set a local property that affects jobs submitted from this thread, such as the\n",
        "        Spark fair scheduler pool.\n",
        "         \n",
        "\n",
        "setSystemProperty \n",
        "        Set a Java system property, such as spark.executor.memory. This must\n",
        "        must be invoked before instantiating SparkContext.\n",
        "         \n",
        "\n",
        "show_profiles  Print the profile stats to stdout  \n",
        "\n",
        "sparkUser \n",
        "        Get SPARK_USER for user who is running SparkContext.\n",
        "         \n",
        "\n",
        "statusTracker \n",
        "        Return :class:`StatusTracker` object\n",
        "         \n",
        "\n",
        "stop \n",
        "        Shut down the SparkContext.\n",
        "         \n",
        "\n",
        "textFile \n",
        "        Read a text file from HDFS, a local file system (available on all\n",
        "        nodes), or any Hadoop-supported file system URI, and return it as an\n",
        "        RDD of Strings.\n",
        "\n",
        "        If use_unicode is False, the strings will be kept as `str` (encoding\n",
        "        as `utf-8`), which is faster and smaller than unicode. (Added in\n",
        "        Spark 1.2)\n",
        "\n",
        "        >>> path = os.path.join(tempdir, \"sample-text.txt\")\n",
        "        >>> with open(path, \"w\") as testFile:\n",
        "        ...    testFile.write(\"Hello world!\")\n",
        "        >>> textFile = sc.textFile(path)\n",
        "        >>> textFile.collect()\n",
        "        [u'Hello world!']\n",
        "         \n",
        "\n",
        "union \n",
        "        Build the union of a list of RDDs.\n",
        "\n",
        "        This supports unions() of RDDs with different serialized formats,\n",
        "        although this forces them to be reserialized using the default\n",
        "        serializer:\n",
        "\n",
        "        >>> path = os.path.join(tempdir, \"union-text.txt\")\n",
        "        >>> with open(path, \"w\") as testFile:\n",
        "        ...    testFile.write(\"Hello\")\n",
        "        >>> textFile = sc.textFile(path)\n",
        "        >>> textFile.collect()\n",
        "        [u'Hello']\n",
        "        >>> parallelized = sc.parallelize([\"World!\"])\n",
        "        >>> sorted(sc.union([textFile, parallelized]).collect())\n",
        "        [u'Hello', 'World!']\n",
        "         \n",
        "\n",
        "wholeTextFiles \n",
        "        Read a directory of text files from HDFS, a local file system\n",
        "        (available on all nodes), or any  Hadoop-supported file system\n",
        "        URI. Each file is read as a single record and returned in a\n",
        "        key-value pair, where the key is the path of each file, the\n",
        "        value is the content of each file.\n",
        "\n",
        "        If use_unicode is False, the strings will be kept as `str` (encoding\n",
        "        as `utf-8`), which is faster and smaller than unicode. (Added in\n",
        "        Spark 1.2)\n",
        "\n",
        "        For example, if you have the following files::\n",
        "\n",
        "          hdfs://a-hdfs-path/part-00000\n",
        "          hdfs://a-hdfs-path/part-00001\n",
        "          ...\n",
        "          hdfs://a-hdfs-path/part-nnnnn\n",
        "\n",
        "        Do C{rdd = sparkContext.wholeTextFiles(\"hdfs://a-hdfs-path\")},\n",
        "        then C{rdd} contains::\n",
        "\n",
        "          (a-hdfs-path/part-00000, its content)\n",
        "          (a-hdfs-path/part-00001, its content)\n",
        "          ...\n",
        "          (a-hdfs-path/part-nnnnn, its content)\n",
        "\n",
        "        NOTE: Small files are preferred, as each file will be loaded\n",
        "        fully in memory.\n",
        "\n",
        "        >>> dirPath = os.path.join(tempdir, \"files\")\n",
        "        >>> os.mkdir(dirPath)\n",
        "        >>> with open(os.path.join(dirPath, \"1.txt\"), \"w\") as file1:\n",
        "        ...    file1.write(\"1\")\n",
        "        >>> with open(os.path.join(dirPath, \"2.txt\"), \"w\") as file2:\n",
        "        ...    file2.write(\"2\")\n",
        "        >>> textFiles = sc.wholeTextFiles(dirPath)\n",
        "        >>> sorted(textFiles.collect())\n",
        "        [(u'.../1.txt', u'1'), (u'.../2.txt', u'2')]\n",
        "         \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize([1])\n",
      "showMethods(rdd)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "aggregate \n",
        "        Aggregate the elements of each partition, and then the results for all\n",
        "        the partitions, using a given combine functions and a neutral \"zero\n",
        "        value.\"\n",
        "\n",
        "        The functions C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
        "        as its result value to avoid object allocation; however, it should not\n",
        "        modify C{t2}.\n",
        "\n",
        "        The first function (seqOp) can return a different result type, U, than\n",
        "        the type of this RDD. Thus, we need one operation for merging a T into\n",
        "        an U and one operation for merging two U\n",
        "\n",
        "        >>> seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
        "        >>> combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
        "        >>> sc.parallelize([1, 2, 3, 4]).aggregate((0, 0), seqOp, combOp)\n",
        "        (10, 4)\n",
        "        >>> sc.parallelize([]).aggregate((0, 0), seqOp, combOp)\n",
        "        (0, 0)\n",
        "         \n",
        "\n",
        "aggregateByKey \n",
        "        Aggregate the values of each key, using given combine functions and a neutral\n",
        "        \"zero value\". This function can return a different result type, U, than the type\n",
        "        of the values in this RDD, V. Thus, we need one operation for merging a V into\n",
        "        a U and one operation for merging two U's, The former operation is used for merging\n",
        "        values within a partition, and the latter is used for merging values between\n",
        "        partitions. To avoid memory allocation, both of these functions are\n",
        "        allowed to modify and return their first argument instead of creating a new U.\n",
        "         \n",
        "\n",
        "cache \n",
        "        Persist this RDD with the default storage level (C{MEMORY_ONLY_SER}).\n",
        "         \n",
        "\n",
        "cartesian \n",
        "        Return the Cartesian product of this RDD and another one, that is, the\n",
        "        RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n",
        "        C{b} is in C{other}.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 2])\n",
        "        >>> sorted(rdd.cartesian(rdd).collect())\n",
        "        [(1, 1), (1, 2), (2, 1), (2, 2)]\n",
        "         \n",
        "\n",
        "checkpoint \n",
        "        Mark this RDD for checkpointing. It will be saved to a file inside the\n",
        "        checkpoint directory set with L{SparkContext.setCheckpointDir()} and\n",
        "        all references to its parent RDDs will be removed. This function must\n",
        "        be called before any job has been executed on this RDD. It is strongly\n",
        "        recommended that this RDD is persisted in memory, otherwise saving it\n",
        "        on a file will require recomputation.\n",
        "         \n",
        "\n",
        "coalesce \n",
        "        Return a new RDD that is reduced into `numPartitions` partitions.\n",
        "\n",
        "        >>> sc.parallelize([1, 2, 3, 4, 5], 3).glom().collect()\n",
        "        [[1], [2, 3], [4, 5]]\n",
        "        >>> sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()\n",
        "        [[1, 2, 3, 4, 5]]\n",
        "         \n",
        "\n",
        "cogroup \n",
        "        For each key k in C{self} or C{other}, return a resulting RDD that\n",
        "        contains a tuple with the list of values for that key in C{self} as\n",
        "        well as C{other}.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "        >>> y = sc.parallelize([(\"a\", 2)])\n",
        "        >>> map((lambda (x,y): (x, (list(y[0]), list(y[1])))), sorted(list(x.cogroup(y).collect())))\n",
        "        [('a', ([1], [2])), ('b', ([4], []))]\n",
        "         \n",
        "\n",
        "collect \n",
        "        Return a list that contains all of the elements in this RDD.\n",
        "         \n",
        "\n",
        "collectAsMap \n",
        "        Return the key-value pairs in this RDD to the master as a dictionary.\n",
        "\n",
        "        >>> m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()\n",
        "        >>> m[1]\n",
        "        2\n",
        "        >>> m[3]\n",
        "        4\n",
        "         \n",
        "\n",
        "combineByKey \n",
        "        Generic function to combine the elements for each key using a custom\n",
        "        set of aggregation functions.\n",
        "\n",
        "        Turns an RDD[(K, V)] into a result of type RDD[(K, C)], for a \"combined\n",
        "        type\" C.  Note that V and C can be different -- for example, one might\n",
        "        group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).\n",
        "\n",
        "        Users provide three functions:\n",
        "\n",
        "            - C{createCombiner}, which turns a V into a C (e.g., creates\n",
        "              a one-element list)\n",
        "            - C{mergeValue}, to merge a V into a C (e.g., adds it to the end of\n",
        "              a list)\n",
        "            - C{mergeCombiners}, to combine two C's into a single one.\n",
        "\n",
        "        In addition, users can control the partitioning of the output RDD.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "        >>> def f(x): return x\n",
        "        >>> def add(a, b): return a + str(b)\n",
        "        >>> sorted(x.combineByKey(str, add, add).collect())\n",
        "        [('a', '11'), ('b', '1')]\n",
        "         \n",
        "\n",
        "count \n",
        "        Return the number of elements in this RDD.\n",
        "\n",
        "        >>> sc.parallelize([2, 3, 4]).count()\n",
        "        3\n",
        "         \n",
        "\n",
        "countApprox \n",
        "        .. note:: Experimental\n",
        "\n",
        "        Approximate version of count() that returns a potentially incomplete\n",
        "        result within a timeout, even if not all tasks have finished.\n",
        "\n",
        "        >>> rdd = sc.parallelize(range(1000), 10)\n",
        "        >>> rdd.countApprox(1000, 1.0)\n",
        "        1000\n",
        "         \n",
        "\n",
        "countApproxDistinct \n",
        "        .. note:: Experimental\n",
        "\n",
        "        Return approximate number of distinct elements in the RDD.\n",
        "\n",
        "        The algorithm used is based on streamlib's implementation of\n",
        "        \"HyperLogLog in Practice: Algorithmic Engineering of a State\n",
        "        of The Art Cardinality Estimation Algorithm\", available\n",
        "        <a href=\"http://dx.doi.org/10.1145/2452376.2452456\">here</a>.\n",
        "\n",
        "        :param relativeSD: Relative accuracy. Smaller values create\n",
        "                           counters that require more space.\n",
        "                           It must be greater than 0.000017.\n",
        "\n",
        "        >>> n = sc.parallelize(range(1000)).map(str).countApproxDistinct()\n",
        "        >>> 950 < n < 1050\n",
        "        True\n",
        "        >>> n = sc.parallelize([i % 20 for i in range(1000)]).countApproxDistinct()\n",
        "        >>> 18 < n < 22\n",
        "        True\n",
        "         \n",
        "\n",
        "countByKey \n",
        "        Count the number of elements for each key, and return the result to the\n",
        "        master as a dictionary.\n",
        "\n",
        "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "        >>> sorted(rdd.countByKey().items())\n",
        "        [('a', 2), ('b', 1)]\n",
        "         \n",
        "\n",
        "countByValue \n",
        "        Return the count of each unique value in this RDD as a dictionary of\n",
        "        (value, count) pairs.\n",
        "\n",
        "        >>> sorted(sc.parallelize([1, 2, 1, 2, 2], 2).countByValue().items())\n",
        "        [(1, 2), (2, 3)]\n",
        "         \n",
        "\n",
        "distinct \n",
        "        Return a new RDD containing the distinct elements in this RDD.\n",
        "\n",
        "        >>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
        "        [1, 2, 3]\n",
        "         \n",
        "\n",
        "filter \n",
        "        Return a new RDD containing only the elements that satisfy a predicate.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "        >>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
        "        [2, 4]\n",
        "         \n",
        "\n",
        "first \n",
        "        Return the first element in this RDD.\n",
        "\n",
        "        >>> sc.parallelize([2, 3, 4]).first()\n",
        "        2\n",
        "        >>> sc.parallelize([]).first()\n",
        "        Traceback (most recent call last):\n",
        "            ...\n",
        "        ValueError: RDD is empty\n",
        "         \n",
        "\n",
        "flatMap \n",
        "        Return a new RDD by first applying a function to all elements of this\n",
        "        RDD, and then flattening the results.\n",
        "\n",
        "        >>> rdd = sc.parallelize([2, 3, 4])\n",
        "        >>> sorted(rdd.flatMap(lambda x: range(1, x)).collect())\n",
        "        [1, 1, 1, 2, 2, 3]\n",
        "        >>> sorted(rdd.flatMap(lambda x: [(x, x), (x, x)]).collect())\n",
        "        [(2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]\n",
        "         \n",
        "\n",
        "flatMapValues \n",
        "        Pass each value in the key-value pair RDD through a flatMap function\n",
        "        without changing the keys; this also retains the original RDD's\n",
        "        partitioning.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
        "        >>> def f(x): return x\n",
        "        >>> x.flatMapValues(f).collect()\n",
        "        [('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]\n",
        "         \n",
        "\n",
        "fold \n",
        "        Aggregate the elements of each partition, and then the results for all\n",
        "        the partitions, using a given associative function and a neutral \"zero\n",
        "        value.\"\n",
        "\n",
        "        The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
        "        as its result value to avoid object allocation; however, it should not\n",
        "        modify C{t2}.\n",
        "\n",
        "        >>> from operator import add\n",
        "        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
        "        15\n",
        "         \n",
        "\n",
        "foldByKey \n",
        "        Merge the values for each key using an associative function \"func\"\n",
        "        and a neutral \"zeroValue\" which may be added to the result an\n",
        "        arbitrary number of times, and must not change the result\n",
        "        (e.g., 0 for addition, or 1 for multiplication.).\n",
        "\n",
        "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "        >>> from operator import add\n",
        "        >>> rdd.foldByKey(0, add).collect()\n",
        "        [('a', 2), ('b', 1)]\n",
        "         \n",
        "\n",
        "foreach \n",
        "        Applies a function to all elements of this RDD.\n",
        "\n",
        "        >>> def f(x): print x\n",
        "        >>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
        "         \n",
        "\n",
        "foreachPartition \n",
        "        Applies a function to each partition of this RDD.\n",
        "\n",
        "        >>> def f(iterator):\n",
        "        ...      for x in iterator:\n",
        "        ...           print x\n",
        "        >>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
        "         \n",
        "\n",
        "fullOuterJoin \n",
        "        Perform a right outer join of C{self} and C{other}.\n",
        "\n",
        "        For each element (k, v) in C{self}, the resulting RDD will either\n",
        "        contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
        "        (k, (v, None)) if no elements in C{other} have key k.\n",
        "\n",
        "        Similarly, for each element (k, w) in C{other}, the resulting RDD will\n",
        "        either contain all pairs (k, (v, w)) for v in C{self}, or the pair\n",
        "        (k, (None, w)) if no elements in C{self} have key k.\n",
        "\n",
        "        Hash-partitions the resulting RDD into the given number of partitions.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "        >>> y = sc.parallelize([(\"a\", 2), (\"c\", 8)])\n",
        "        >>> sorted(x.fullOuterJoin(y).collect())\n",
        "        [('a', (1, 2)), ('b', (4, None)), ('c', (None, 8))]\n",
        "         \n",
        "\n",
        "getCheckpointFile \n",
        "        Gets the name of the file to which this RDD was checkpointed\n",
        "         \n",
        "\n",
        "getNumPartitions \n",
        "        Returns the number of partitions in RDD\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
        "        >>> rdd.getNumPartitions()\n",
        "        2\n",
        "         \n",
        "\n",
        "getStorageLevel \n",
        "        Get the RDD's current storage level.\n",
        "\n",
        "        >>> rdd1 = sc.parallelize([1,2])\n",
        "        >>> rdd1.getStorageLevel()\n",
        "        StorageLevel(False, False, False, False, 1)\n",
        "        >>> print(rdd1.getStorageLevel())\n",
        "        Serialized 1x Replicated\n",
        "         \n",
        "\n",
        "glom \n",
        "        Return an RDD created by coalescing all elements within each partition\n",
        "        into a list.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
        "        >>> sorted(rdd.glom().collect())\n",
        "        [[1, 2], [3, 4]]\n",
        "         \n",
        "\n",
        "groupBy \n",
        "        Return an RDD of grouped items.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
        "        >>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
        "        >>> sorted([(x, sorted(y)) for (x, y) in result])\n",
        "        [(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
        "         \n",
        "\n",
        "groupByKey \n",
        "        Group the values for each key in the RDD into a single sequence.\n",
        "        Hash-partitions the resulting RDD with into numPartitions partitions.\n",
        "\n",
        "        Note: If you are grouping in order to perform an aggregation (such as a\n",
        "        sum or average) over each key, using reduceByKey or aggregateByKey will\n",
        "        provide much better performance.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "        >>> map((lambda (x,y): (x, list(y))), sorted(x.groupByKey().collect()))\n",
        "        [('a', [1, 1]), ('b', [1])]\n",
        "         \n",
        "\n",
        "groupWith \n",
        "        Alias for cogroup but with support for multiple RDDs.\n",
        "\n",
        "        >>> w = sc.parallelize([(\"a\", 5), (\"b\", 6)])\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "        >>> y = sc.parallelize([(\"a\", 2)])\n",
        "        >>> z = sc.parallelize([(\"b\", 42)])\n",
        "        >>> map((lambda (x,y): (x, (list(y[0]), list(y[1]), list(y[2]), list(y[3])))),                 sorted(list(w.groupWith(x, y, z).collect())))\n",
        "        [('a', ([5], [1], [2], [])), ('b', ([6], [4], [], [42]))]\n",
        "\n",
        "         \n",
        "\n",
        "histogram \n",
        "        Compute a histogram using the provided buckets. The buckets\n",
        "        are all open to the right except for the last which is closed.\n",
        "        e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
        "        which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
        "        and 50 we would have a histogram of 1,0,1.\n",
        "\n",
        "        If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
        "        this can be switched from an O(log n) inseration to O(1) per\n",
        "        element(where n = # buckets).\n",
        "\n",
        "        Buckets must be sorted and not contain any duplicates, must be\n",
        "        at least two elements.\n",
        "\n",
        "        If `buckets` is a number, it will generates buckets which are\n",
        "        evenly spaced between the minimum and maximum of the RDD. For\n",
        "        example, if the min value is 0 and the max is 100, given buckets\n",
        "        as 2, the resulting buckets will be [0,50) [50,100]. buckets must\n",
        "        be at least 1 If the RDD contains infinity, NaN throws an exception\n",
        "        If the elements in RDD do not vary (max == min) always returns\n",
        "        a single bucket.\n",
        "\n",
        "        It will return an tuple of buckets and histogram.\n",
        "\n",
        "        >>> rdd = sc.parallelize(range(51))\n",
        "        >>> rdd.histogram(2)\n",
        "        ([0, 25, 50], [25, 26])\n",
        "        >>> rdd.histogram([0, 5, 25, 50])\n",
        "        ([0, 5, 25, 50], [5, 20, 26])\n",
        "        >>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
        "        ([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
        "        >>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
        "        >>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
        "        (('a', 'b', 'c'), [2, 2])\n",
        "         \n",
        "\n",
        "id \n",
        "        A unique ID for this RDD (within its SparkContext).\n",
        "         \n",
        "\n",
        "intersection \n",
        "        Return the intersection of this RDD and another one. The output will\n",
        "        not contain any duplicate elements, even if the input RDDs did.\n",
        "\n",
        "        Note that this method performs a shuffle internally.\n",
        "\n",
        "        >>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
        "        >>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
        "        >>> rdd1.intersection(rdd2).collect()\n",
        "        [1, 2, 3]\n",
        "         \n",
        "\n",
        "isCheckpointed \n",
        "        Return whether this RDD has been checkpointed or not\n",
        "         \n",
        "\n",
        "isEmpty \n",
        "        Returns true if and only if the RDD contains no elements at all. Note that an RDD\n",
        "        may be empty even when it has at least 1 partition.\n",
        "\n",
        "        >>> sc.parallelize([]).isEmpty()\n",
        "        True\n",
        "        >>> sc.parallelize([1]).isEmpty()\n",
        "        False\n",
        "         \n",
        "\n",
        "join \n",
        "        Return an RDD containing all pairs of elements with matching keys in\n",
        "        C{self} and C{other}.\n",
        "\n",
        "        Each pair of elements will be returned as a (k, (v1, v2)) tuple, where\n",
        "        (k, v1) is in C{self} and (k, v2) is in C{other}.\n",
        "\n",
        "        Performs a hash join across the cluster.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "        >>> y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
        "        >>> sorted(x.join(y).collect())\n",
        "        [('a', (1, 2)), ('a', (1, 3))]\n",
        "         \n",
        "\n",
        "keyBy \n",
        "        Creates tuples of the elements in this RDD by applying C{f}.\n",
        "\n",
        "        >>> x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)\n",
        "        >>> y = sc.parallelize(zip(range(0,5), range(0,5)))\n",
        "        >>> map((lambda (x,y): (x, (list(y[0]), (list(y[1]))))), sorted(x.cogroup(y).collect()))\n",
        "        [(0, ([0], [0])), (1, ([1], [1])), (2, ([], [2])), (3, ([], [3])), (4, ([2], [4]))]\n",
        "         \n",
        "\n",
        "keys \n",
        "        Return an RDD with the keys of each tuple.\n",
        "\n",
        "        >>> m = sc.parallelize([(1, 2), (3, 4)]).keys()\n",
        "        >>> m.collect()\n",
        "        [1, 3]\n",
        "         \n",
        "\n",
        "leftOuterJoin \n",
        "        Perform a left outer join of C{self} and C{other}.\n",
        "\n",
        "        For each element (k, v) in C{self}, the resulting RDD will either\n",
        "        contain all pairs (k, (v, w)) for w in C{other}, or the pair\n",
        "        (k, (v, None)) if no elements in C{other} have key k.\n",
        "\n",
        "        Hash-partitions the resulting RDD into the given number of partitions.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "        >>> y = sc.parallelize([(\"a\", 2)])\n",
        "        >>> sorted(x.leftOuterJoin(y).collect())\n",
        "        [('a', (1, 2)), ('b', (4, None))]\n",
        "         \n",
        "\n",
        "lookup \n",
        "        Return the list of values in the RDD for key `key`. This operation\n",
        "        is done efficiently if the RDD has a known partitioner by only\n",
        "        searching the partition that the key maps to.\n",
        "\n",
        "        >>> l = range(1000)\n",
        "        >>> rdd = sc.parallelize(zip(l, l), 10)\n",
        "        >>> rdd.lookup(42)  # slow\n",
        "        [42]\n",
        "        >>> sorted = rdd.sortByKey()\n",
        "        >>> sorted.lookup(42)  # fast\n",
        "        [42]\n",
        "        >>> sorted.lookup(1024)\n",
        "        []\n",
        "         \n",
        "\n",
        "map \n",
        "        Return a new RDD by applying a function to each element of this RDD.\n",
        "\n",
        "        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
        "        >>> sorted(rdd.map(lambda x: (x, 1)).collect())\n",
        "        [('a', 1), ('b', 1), ('c', 1)]\n",
        "         \n",
        "\n",
        "mapPartitions \n",
        "        Return a new RDD by applying a function to each partition of this RDD.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
        "        >>> def f(iterator): yield sum(iterator)\n",
        "        >>> rdd.mapPartitions(f).collect()\n",
        "        [3, 7]\n",
        "         \n",
        "\n",
        "mapPartitionsWithIndex \n",
        "        Return a new RDD by applying a function to each partition of this RDD,\n",
        "        while tracking the index of the original partition.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
        "        >>> def f(splitIndex, iterator): yield splitIndex\n",
        "        >>> rdd.mapPartitionsWithIndex(f).sum()\n",
        "        6\n",
        "         \n",
        "\n",
        "mapPartitionsWithSplit \n",
        "        Deprecated: use mapPartitionsWithIndex instead.\n",
        "\n",
        "        Return a new RDD by applying a function to each partition of this RDD,\n",
        "        while tracking the index of the original partition.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 2, 3, 4], 4)\n",
        "        >>> def f(splitIndex, iterator): yield splitIndex\n",
        "        >>> rdd.mapPartitionsWithSplit(f).sum()\n",
        "        6\n",
        "         \n",
        "\n",
        "mapValues \n",
        "        Pass each value in the key-value pair RDD through a map function\n",
        "        without changing the keys; this also retains the original RDD's\n",
        "        partitioning.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
        "        >>> def f(x): return len(x)\n",
        "        >>> x.mapValues(f).collect()\n",
        "        [('a', 3), ('b', 1)]\n",
        "         \n",
        "\n",
        "max \n",
        "        Find the maximum item in this RDD.\n",
        "\n",
        "        :param key: A function used to generate key for comparing\n",
        "\n",
        "        >>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
        "        >>> rdd.max()\n",
        "        43.0\n",
        "        >>> rdd.max(key=str)\n",
        "        5.0\n",
        "         \n",
        "\n",
        "mean \n",
        "        Compute the mean of this RDD's elements.\n",
        "\n",
        "        >>> sc.parallelize([1, 2, 3]).mean()\n",
        "        2.0\n",
        "         \n",
        "\n",
        "meanApprox \n",
        "        .. note:: Experimental\n",
        "\n",
        "        Approximate operation to return the mean within a timeout\n",
        "        or meet the confidence.\n",
        "\n",
        "        >>> rdd = sc.parallelize(range(1000), 10)\n",
        "        >>> r = sum(xrange(1000)) / 1000.0\n",
        "        >>> (rdd.meanApprox(1000) - r) / r < 0.05\n",
        "        True\n",
        "         \n",
        "\n",
        "min \n",
        "        Find the minimum item in this RDD.\n",
        "\n",
        "        :param key: A function used to generate key for comparing\n",
        "\n",
        "        >>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
        "        >>> rdd.min()\n",
        "        2.0\n",
        "        >>> rdd.min(key=str)\n",
        "        10.0\n",
        "         \n",
        "\n",
        "name \n",
        "        Return the name of this RDD.\n",
        "         \n",
        "\n",
        "partitionBy \n",
        "        Return a copy of the RDD partitioned using the specified partitioner.\n",
        "\n",
        "        >>> pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1]).map(lambda x: (x, x))\n",
        "        >>> sets = pairs.partitionBy(2).glom().collect()\n",
        "        >>> set(sets[0]).intersection(set(sets[1]))\n",
        "        set([])\n",
        "         \n",
        "\n",
        "persist \n",
        "        Set this RDD's storage level to persist its values across operations\n",
        "        after the first time it is computed. This can only be used to assign\n",
        "        a new storage level if the RDD does not have a storage level set yet.\n",
        "        If no storage level is specified defaults to (C{MEMORY_ONLY_SER}).\n",
        "\n",
        "        >>> rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
        "        >>> rdd.persist().is_cached\n",
        "        True\n",
        "         \n",
        "\n",
        "pipe \n",
        "        Return an RDD created by piping elements to a forked external process.\n",
        "\n",
        "        >>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
        "        ['1', '2', '', '3']\n",
        "         \n",
        "\n",
        "randomSplit \n",
        "        Randomly splits this RDD with the provided weights.\n",
        "\n",
        "        :param weights: weights for splits, will be normalized if they don't sum to 1\n",
        "        :param seed: random seed\n",
        "        :return: split RDDs in a list\n",
        "\n",
        "        >>> rdd = sc.parallelize(range(5), 1)\n",
        "        >>> rdd1, rdd2 = rdd.randomSplit([2, 3], 17)\n",
        "        >>> rdd1.collect()\n",
        "        [1, 3]\n",
        "        >>> rdd2.collect()\n",
        "        [0, 2, 4]\n",
        "         \n",
        "\n",
        "reduce \n",
        "        Reduces the elements of this RDD using the specified commutative and\n",
        "        associative binary operator. Currently reduces partitions locally.\n",
        "\n",
        "        >>> from operator import add\n",
        "        >>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
        "        15\n",
        "        >>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
        "        10\n",
        "        >>> sc.parallelize([]).reduce(add)\n",
        "        Traceback (most recent call last):\n",
        "            ...\n",
        "        ValueError: Can not reduce() empty RDD\n",
        "         \n",
        "\n",
        "reduceByKey \n",
        "        Merge the values for each key using an associative reduce function.\n",
        "\n",
        "        This will also perform the merging locally on each mapper before\n",
        "        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
        "\n",
        "        Output will be hash-partitioned with C{numPartitions} partitions, or\n",
        "        the default parallelism level if C{numPartitions} is not specified.\n",
        "\n",
        "        >>> from operator import add\n",
        "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "        >>> sorted(rdd.reduceByKey(add).collect())\n",
        "        [('a', 2), ('b', 1)]\n",
        "         \n",
        "\n",
        "reduceByKeyLocally \n",
        "        Merge the values for each key using an associative reduce function, but\n",
        "        return the results immediately to the master as a dictionary.\n",
        "\n",
        "        This will also perform the merging locally on each mapper before\n",
        "        sending results to a reducer, similarly to a \"combiner\" in MapReduce.\n",
        "\n",
        "        >>> from operator import add\n",
        "        >>> rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
        "        >>> sorted(rdd.reduceByKeyLocally(add).items())\n",
        "        [('a', 2), ('b', 1)]\n",
        "         \n",
        "\n",
        "repartition \n",
        "         Return a new RDD that has exactly numPartitions partitions.\n",
        "\n",
        "         Can increase or decrease the level of parallelism in this RDD.\n",
        "         Internally, this uses a shuffle to redistribute data.\n",
        "         If you are decreasing the number of partitions in this RDD, consider\n",
        "         using `coalesce`, which can avoid performing a shuffle.\n",
        "\n",
        "         >>> rdd = sc.parallelize([1,2,3,4,5,6,7], 4)\n",
        "         >>> sorted(rdd.glom().collect())\n",
        "         [[1], [2, 3], [4, 5], [6, 7]]\n",
        "         >>> len(rdd.repartition(2).glom().collect())\n",
        "         2\n",
        "         >>> len(rdd.repartition(10).glom().collect())\n",
        "         10\n",
        "         \n",
        "\n",
        "repartitionAndSortWithinPartitions \n",
        "        Repartition the RDD according to the given partitioner and, within each resulting partition,\n",
        "        sort records by their keys.\n",
        "\n",
        "        >>> rdd = sc.parallelize([(0, 5), (3, 8), (2, 6), (0, 8), (3, 8), (1, 3)])\n",
        "        >>> rdd2 = rdd.repartitionAndSortWithinPartitions(2, lambda x: x % 2, 2)\n",
        "        >>> rdd2.glom().collect()\n",
        "        [[(0, 5), (0, 8), (2, 6)], [(1, 3), (3, 8), (3, 8)]]\n",
        "         \n",
        "\n",
        "rightOuterJoin \n",
        "        Perform a right outer join of C{self} and C{other}.\n",
        "\n",
        "        For each element (k, w) in C{other}, the resulting RDD will either\n",
        "        contain all pairs (k, (v, w)) for v in this, or the pair (k, (None, w))\n",
        "        if no elements in C{self} have key k.\n",
        "\n",
        "        Hash-partitions the resulting RDD into the given number of partitions.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
        "        >>> y = sc.parallelize([(\"a\", 2)])\n",
        "        >>> sorted(y.rightOuterJoin(x).collect())\n",
        "        [('a', (2, 1)), ('b', (None, 4))]\n",
        "         \n",
        "\n",
        "sample \n",
        "        Return a sampled subset of this RDD.\n",
        "\n",
        "        >>> rdd = sc.parallelize(range(100), 4)\n",
        "        >>> rdd.sample(False, 0.1, 81).count()\n",
        "        10\n",
        "         \n",
        "\n",
        "sampleByKey \n",
        "        Return a subset of this RDD sampled by key (via stratified sampling).\n",
        "        Create a sample of this RDD using variable sampling rates for\n",
        "        different keys as specified by fractions, a key to sampling rate map.\n",
        "\n",
        "        >>> fractions = {\"a\": 0.2, \"b\": 0.1}\n",
        "        >>> rdd = sc.parallelize(fractions.keys()).cartesian(sc.parallelize(range(0, 1000)))\n",
        "        >>> sample = dict(rdd.sampleByKey(False, fractions, 2).groupByKey().collect())\n",
        "        >>> 100 < len(sample[\"a\"]) < 300 and 50 < len(sample[\"b\"]) < 150\n",
        "        True\n",
        "        >>> max(sample[\"a\"]) <= 999 and min(sample[\"a\"]) >= 0\n",
        "        True\n",
        "        >>> max(sample[\"b\"]) <= 999 and min(sample[\"b\"]) >= 0\n",
        "        True\n",
        "         \n",
        "\n",
        "sampleStdev \n",
        "        Compute the sample standard deviation of this RDD's elements (which\n",
        "        corrects for bias in estimating the standard deviation by dividing by\n",
        "        N-1 instead of N).\n",
        "\n",
        "        >>> sc.parallelize([1, 2, 3]).sampleStdev()\n",
        "        1.0\n",
        "         \n",
        "\n",
        "sampleVariance \n",
        "        Compute the sample variance of this RDD's elements (which corrects\n",
        "        for bias in estimating the variance by dividing by N-1 instead of N).\n",
        "\n",
        "        >>> sc.parallelize([1, 2, 3]).sampleVariance()\n",
        "        1.0\n",
        "         \n",
        "\n",
        "saveAsHadoopDataset \n",
        "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
        "        system, using the old Hadoop OutputFormat API (mapred package). Keys/values are\n",
        "        converted for output using either user specified converters or, by default,\n",
        "        L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
        "\n",
        "        :param conf: Hadoop job configuration, passed in as a dict\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "         \n",
        "\n",
        "saveAsHadoopFile \n",
        "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
        "        system, using the old Hadoop OutputFormat API (mapred package). Key and value types\n",
        "        will be inferred if not specified. Keys and values are converted for output using either\n",
        "        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
        "        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
        "        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
        "\n",
        "        :param path: path to Hadoop file\n",
        "        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
        "               (e.g. \"org.apache.hadoop.mapred.SequenceFileOutputFormat\")\n",
        "        :param keyClass: fully qualified classname of key Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
        "        :param valueClass: fully qualified classname of value Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "        :param conf: (None by default)\n",
        "        :param compressionCodecClass: (None by default)\n",
        "         \n",
        "\n",
        "saveAsNewAPIHadoopDataset \n",
        "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
        "        system, using the new Hadoop OutputFormat API (mapreduce package). Keys/values are\n",
        "        converted for output using either user specified converters or, by default,\n",
        "        L{org.apache.spark.api.python.JavaToWritableConverter}.\n",
        "\n",
        "        :param conf: Hadoop job configuration, passed in as a dict\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "         \n",
        "\n",
        "saveAsNewAPIHadoopFile \n",
        "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
        "        system, using the new Hadoop OutputFormat API (mapreduce package). Key and value types\n",
        "        will be inferred if not specified. Keys and values are converted for output using either\n",
        "        user specified converters or L{org.apache.spark.api.python.JavaToWritableConverter}. The\n",
        "        C{conf} is applied on top of the base Hadoop conf associated with the SparkContext\n",
        "        of this RDD to create a merged Hadoop MapReduce job configuration for saving the data.\n",
        "\n",
        "        :param path: path to Hadoop file\n",
        "        :param outputFormatClass: fully qualified classname of Hadoop OutputFormat\n",
        "               (e.g. \"org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\")\n",
        "        :param keyClass: fully qualified classname of key Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.IntWritable\", None by default)\n",
        "        :param valueClass: fully qualified classname of value Writable class\n",
        "               (e.g. \"org.apache.hadoop.io.Text\", None by default)\n",
        "        :param keyConverter: (None by default)\n",
        "        :param valueConverter: (None by default)\n",
        "        :param conf: Hadoop job configuration, passed in as a dict (None by default)\n",
        "         \n",
        "\n",
        "saveAsPickleFile \n",
        "        Save this RDD as a SequenceFile of serialized objects. The serializer\n",
        "        used is L{pyspark.serializers.PickleSerializer}, default batch size\n",
        "        is 10.\n",
        "\n",
        "        >>> tmpFile = NamedTemporaryFile(delete=True)\n",
        "        >>> tmpFile.close()\n",
        "        >>> sc.parallelize([1, 2, 'spark', 'rdd']).saveAsPickleFile(tmpFile.name, 3)\n",
        "        >>> sorted(sc.pickleFile(tmpFile.name, 5).collect())\n",
        "        [1, 2, 'rdd', 'spark']\n",
        "         \n",
        "\n",
        "saveAsSequenceFile \n",
        "        Output a Python RDD of key-value pairs (of form C{RDD[(K, V)]}) to any Hadoop file\n",
        "        system, using the L{org.apache.hadoop.io.Writable} types that we convert from the\n",
        "        RDD's key and value types. The mechanism is as follows:\n",
        "\n",
        "            1. Pyrolite is used to convert pickled Python RDD into RDD of Java objects.\n",
        "            2. Keys and values of this Java RDD are converted to Writables and written out.\n",
        "\n",
        "        :param path: path to sequence file\n",
        "        :param compressionCodecClass: (None by default)\n",
        "         \n",
        "\n",
        "saveAsTextFile \n",
        "        Save this RDD as a text file, using string representations of elements.\n",
        "\n",
        "        @param path: path to text file\n",
        "        @param compressionCodecClass: (None by default) string i.e.\n",
        "            \"org.apache.hadoop.io.compress.GzipCodec\"\n",
        "\n",
        "        >>> tempFile = NamedTemporaryFile(delete=True)\n",
        "        >>> tempFile.close()\n",
        "        >>> sc.parallelize(range(10)).saveAsTextFile(tempFile.name)\n",
        "        >>> from fileinput import input\n",
        "        >>> from glob import glob\n",
        "        >>> ''.join(sorted(input(glob(tempFile.name + \"/part-0000*\"))))\n",
        "        '0\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n'\n",
        "\n",
        "        Empty lines are tolerated when saving to text files.\n",
        "\n",
        "        >>> tempFile2 = NamedTemporaryFile(delete=True)\n",
        "        >>> tempFile2.close()\n",
        "        >>> sc.parallelize(['', 'foo', '', 'bar', '']).saveAsTextFile(tempFile2.name)\n",
        "        >>> ''.join(sorted(input(glob(tempFile2.name + \"/part-0000*\"))))\n",
        "        '\\n\\n\\nbar\\nfoo\\n'\n",
        "\n",
        "        Using compressionCodecClass\n",
        "\n",
        "        >>> tempFile3 = NamedTemporaryFile(delete=True)\n",
        "        >>> tempFile3.close()\n",
        "        >>> codec = \"org.apache.hadoop.io.compress.GzipCodec\"\n",
        "        >>> sc.parallelize(['foo', 'bar']).saveAsTextFile(tempFile3.name, codec)\n",
        "        >>> from fileinput import input, hook_compressed\n",
        "        >>> ''.join(sorted(input(glob(tempFile3.name + \"/part*.gz\"), openhook=hook_compressed)))\n",
        "        'bar\\nfoo\\n'\n",
        "         \n",
        "\n",
        "setName \n",
        "        Assign a name to this RDD.\n",
        "\n",
        "        >>> rdd1 = sc.parallelize([1,2])\n",
        "        >>> rdd1.setName('RDD1').name()\n",
        "        'RDD1'\n",
        "         \n",
        "\n",
        "sortBy \n",
        "        Sorts this RDD by the given keyfunc\n",
        "\n",
        "        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
        "        >>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
        "        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
        "        >>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
        "        [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
        "         \n",
        "\n",
        "sortByKey \n",
        "        Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
        "        # noqa\n",
        "\n",
        "        >>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
        "        >>> sc.parallelize(tmp).sortByKey().first()\n",
        "        ('1', 3)\n",
        "        >>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
        "        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
        "        >>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
        "        [('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
        "        >>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
        "        >>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
        "        >>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
        "        [('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
        "         \n",
        "\n",
        "stats \n",
        "        Return a L{StatCounter} object that captures the mean, variance\n",
        "        and count of the RDD's elements in one operation.\n",
        "         \n",
        "\n",
        "stdev \n",
        "        Compute the standard deviation of this RDD's elements.\n",
        "\n",
        "        >>> sc.parallelize([1, 2, 3]).stdev()\n",
        "        0.816...\n",
        "         \n",
        "\n",
        "subtract \n",
        "        Return each value in C{self} that is not contained in C{other}.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
        "        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
        "        >>> sorted(x.subtract(y).collect())\n",
        "        [('a', 1), ('b', 4), ('b', 5)]\n",
        "         \n",
        "\n",
        "subtractByKey \n",
        "        Return each (key, value) pair in C{self} that has no pair with matching\n",
        "        key in C{other}.\n",
        "\n",
        "        >>> x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 2)])\n",
        "        >>> y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
        "        >>> sorted(x.subtractByKey(y).collect())\n",
        "        [('b', 4), ('b', 5)]\n",
        "         \n",
        "\n",
        "sum \n",
        "        Add up the elements in this RDD.\n",
        "\n",
        "        >>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
        "        6.0\n",
        "         \n",
        "\n",
        "sumApprox \n",
        "        .. note:: Experimental\n",
        "\n",
        "        Approximate operation to return the sum within a timeout\n",
        "        or meet the confidence.\n",
        "\n",
        "        >>> rdd = sc.parallelize(range(1000), 10)\n",
        "        >>> r = sum(xrange(1000))\n",
        "        >>> (rdd.sumApprox(1000) - r) / r < 0.05\n",
        "        True\n",
        "         \n",
        "\n",
        "take \n",
        "        Take the first num elements of the RDD.\n",
        "\n",
        "        It works by first scanning one partition, and use the results from\n",
        "        that partition to estimate the number of additional partitions needed\n",
        "        to satisfy the limit.\n",
        "\n",
        "        Translated from the Scala implementation in RDD#take().\n",
        "\n",
        "        >>> sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)\n",
        "        [2, 3]\n",
        "        >>> sc.parallelize([2, 3, 4, 5, 6]).take(10)\n",
        "        [2, 3, 4, 5, 6]\n",
        "        >>> sc.parallelize(range(100), 100).filter(lambda x: x > 90).take(3)\n",
        "        [91, 92, 93]\n",
        "         \n",
        "\n",
        "takeOrdered \n",
        "        Get the N elements from a RDD ordered in ascending order or as\n",
        "        specified by the optional key function.\n",
        "\n",
        "        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)\n",
        "        [1, 2, 3, 4, 5, 6]\n",
        "        >>> sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7], 2).takeOrdered(6, key=lambda x: -x)\n",
        "        [10, 9, 7, 6, 5, 4]\n",
        "         \n",
        "\n",
        "takeSample \n",
        "        Return a fixed-size sampled subset of this RDD.\n",
        "\n",
        "        >>> rdd = sc.parallelize(range(0, 10))\n",
        "        >>> len(rdd.takeSample(True, 20, 1))\n",
        "        20\n",
        "        >>> len(rdd.takeSample(False, 5, 2))\n",
        "        5\n",
        "        >>> len(rdd.takeSample(False, 15, 3))\n",
        "        10\n",
        "         \n",
        "\n",
        "toDF \n",
        "        Converts current :class:`RDD` into a :class:`DataFrame`\n",
        "\n",
        "        This is a shorthand for ``sqlContext.createDataFrame(rdd, schema, sampleRatio)``\n",
        "\n",
        "        :param schema: a StructType or list of names of columns\n",
        "        :param samplingRatio: the sample ratio of rows used for inferring\n",
        "        :return: a DataFrame\n",
        "\n",
        "        >>> rdd.toDF().collect()\n",
        "        [Row(name=u'Alice', age=1)]\n",
        "         \n",
        "\n",
        "toDebugString \n",
        "        A description of this RDD and its recursive dependencies for debugging.\n",
        "         \n",
        "\n",
        "toLocalIterator \n",
        "        Return an iterator that contains all of the elements in this RDD.\n",
        "        The iterator will consume as much memory as the largest partition in this RDD.\n",
        "        >>> rdd = sc.parallelize(range(10))\n",
        "        >>> [x for x in rdd.toLocalIterator()]\n",
        "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "         \n",
        "\n",
        "top \n",
        "        Get the top N elements from a RDD.\n",
        "\n",
        "        Note: It returns the list sorted in descending order.\n",
        "\n",
        "        >>> sc.parallelize([10, 4, 2, 12, 3]).top(1)\n",
        "        [12]\n",
        "        >>> sc.parallelize([2, 3, 4, 5, 6], 2).top(2)\n",
        "        [6, 5]\n",
        "        >>> sc.parallelize([10, 4, 2, 12, 3]).top(3, key=str)\n",
        "        [4, 3, 2]\n",
        "         \n",
        "\n",
        "treeAggregate \n",
        "        Aggregates the elements of this RDD in a multi-level tree\n",
        "        pattern.\n",
        "\n",
        "        :param depth: suggested depth of the tree (default: 2)\n",
        "\n",
        "        >>> add = lambda x, y: x + y\n",
        "        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
        "        >>> rdd.treeAggregate(0, add, add)\n",
        "        -5\n",
        "        >>> rdd.treeAggregate(0, add, add, 1)\n",
        "        -5\n",
        "        >>> rdd.treeAggregate(0, add, add, 2)\n",
        "        -5\n",
        "        >>> rdd.treeAggregate(0, add, add, 5)\n",
        "        -5\n",
        "        >>> rdd.treeAggregate(0, add, add, 10)\n",
        "        -5\n",
        "         \n",
        "\n",
        "treeReduce \n",
        "        Reduces the elements of this RDD in a multi-level tree pattern.\n",
        "\n",
        "        :param depth: suggested depth of the tree (default: 2)\n",
        "\n",
        "        >>> add = lambda x, y: x + y\n",
        "        >>> rdd = sc.parallelize([-5, -4, -3, -2, -1, 1, 2, 3, 4], 10)\n",
        "        >>> rdd.treeReduce(add)\n",
        "        -5\n",
        "        >>> rdd.treeReduce(add, 1)\n",
        "        -5\n",
        "        >>> rdd.treeReduce(add, 2)\n",
        "        -5\n",
        "        >>> rdd.treeReduce(add, 5)\n",
        "        -5\n",
        "        >>> rdd.treeReduce(add, 10)\n",
        "        -5\n",
        "         \n",
        "\n",
        "union \n",
        "        Return the union of this RDD and another one.\n",
        "\n",
        "        >>> rdd = sc.parallelize([1, 1, 2, 3])\n",
        "        >>> rdd.union(rdd).collect()\n",
        "        [1, 1, 2, 3, 1, 1, 2, 3]\n",
        "         \n",
        "\n",
        "unpersist \n",
        "        Mark the RDD as non-persistent, and remove all blocks for it from\n",
        "        memory and disk.\n",
        "         \n",
        "\n",
        "values \n",
        "        Return an RDD with the values of each tuple.\n",
        "\n",
        "        >>> m = sc.parallelize([(1, 2), (3, 4)]).values()\n",
        "        >>> m.collect()\n",
        "        [2, 4]\n",
        "         \n",
        "\n",
        "variance \n",
        "        Compute the variance of this RDD's elements.\n",
        "\n",
        "        >>> sc.parallelize([1, 2, 3]).variance()\n",
        "        0.666...\n",
        "         \n",
        "\n",
        "zip \n",
        "        Zips this RDD with another one, returning key-value pairs with the\n",
        "        first element in each RDD second element in each RDD, etc. Assumes\n",
        "        that the two RDDs have the same number of partitions and the same\n",
        "        number of elements in each partition (e.g. one was made through\n",
        "        a map on the other).\n",
        "\n",
        "        >>> x = sc.parallelize(range(0,5))\n",
        "        >>> y = sc.parallelize(range(1000, 1005))\n",
        "        >>> x.zip(y).collect()\n",
        "        [(0, 1000), (1, 1001), (2, 1002), (3, 1003), (4, 1004)]\n",
        "         \n",
        "\n",
        "zipWithIndex \n",
        "        Zips this RDD with its element indices.\n",
        "\n",
        "        The ordering is first based on the partition index and then the\n",
        "        ordering of items within each partition. So the first item in\n",
        "        the first partition gets index 0, and the last item in the last\n",
        "        partition receives the largest index.\n",
        "\n",
        "        This method needs to trigger a spark job when this RDD contains\n",
        "        more than one partitions.\n",
        "\n",
        "        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\"], 3).zipWithIndex().collect()\n",
        "        [('a', 0), ('b', 1), ('c', 2), ('d', 3)]\n",
        "         \n",
        "\n",
        "zipWithUniqueId \n",
        "        Zips this RDD with generated unique Long ids.\n",
        "\n",
        "        Items in the kth partition will get ids k, n+k, 2*n+k, ..., where\n",
        "        n is the number of partitions. So there may exist gaps, but this\n",
        "        method won't trigger a spark job, which is different from\n",
        "        L{zipWithIndex}\n",
        "\n",
        "        >>> sc.parallelize([\"a\", \"b\", \"c\", \"d\", \"e\"], 3).zipWithUniqueId().collect()\n",
        "        [('a', 0), ('b', 1), ('c', 4), ('d', 2), ('e', 5)]\n",
        "         \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}